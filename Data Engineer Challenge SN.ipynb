{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer Challenge "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Library import\n",
    "We import all the required Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T12:28:20.698908Z",
     "start_time": "2021-10-28T12:28:19.939116Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 30\n",
    "\n",
    "\n",
    "# Autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "    \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T15:00:02.929547Z",
     "start_time": "2021-10-28T15:00:02.863408Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data import\n",
    "We retrieve all the required data for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T15:53:45.754360Z",
     "start_time": "2021-10-28T15:53:45.678671Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "site_0 = pd.read_csv(r'C:\\Users\\Sam.Winter\\OneDrive - Wilmington\\Documents\\Physio\\Springer Nature\\site_0.csv')\n",
    "site_1 = pd.read_csv(r'C:\\Users\\Sam.Winter\\OneDrive - Wilmington\\Documents\\Physio\\Springer Nature\\site_1.csv')\n",
    "site_2 = pd.read_csv(r'C:\\Users\\Sam.Winter\\OneDrive - Wilmington\\Documents\\Physio\\Springer Nature\\site_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "Put here the core of the notebook. Feel free di further split this section into subsections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T15:53:46.785128Z",
     "start_time": "2021-10-28T15:53:46.724410Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "site_0['Site'] = 0\n",
    "site_1['Site'] = 1\n",
    "site_2['Site'] = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T15:53:47.899162Z",
     "start_time": "2021-10-28T15:53:47.808407Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View time is always labeled with historic dates, as we'd hope! All views appear to be within the same date, within the same hour\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-224-0477c23bd118>:10: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  allViews.Timestamp = pd.to_datetime(allViews.timestamp,format='%y/%m/%d %H:%M:%S')\n",
      "<ipython-input-224-0477c23bd118>:11: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  assert (allViews.Timestamp > pd.datetime.now()).sum() < allViews.Timestamp.notna().count(), \"View time appears to contain labels in the future\"\n",
      "<ipython-input-224-0477c23bd118>:13: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  if (allViews.Timestamp > pd.datetime.now()).sum() > allViews.Timestamp.notna().count():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Second</th>\n",
       "      <th>Minute</th>\n",
       "      <th>UserId</th>\n",
       "      <th>site</th>\n",
       "      <th>siteViews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3282.000000</td>\n",
       "      <td>3282.000000</td>\n",
       "      <td>3282.000000</td>\n",
       "      <td>3282.000000</td>\n",
       "      <td>3282.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.723035</td>\n",
       "      <td>16.809263</td>\n",
       "      <td>549.040829</td>\n",
       "      <td>0.946984</td>\n",
       "      <td>1098.628885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.165537</td>\n",
       "      <td>10.272760</td>\n",
       "      <td>256.610818</td>\n",
       "      <td>0.813775</td>\n",
       "      <td>70.729309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1004.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1004.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>534.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>59.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>998.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1178.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Second       Minute       UserId         site    siteViews\n",
       "count  3282.000000  3282.000000  3282.000000  3282.000000  3282.000000\n",
       "mean     29.723035    16.809263   549.040829     0.946984  1098.628885\n",
       "std      17.165537    10.272760   256.610818     0.813775    70.729309\n",
       "min       0.000000     0.000000   100.000000     0.000000  1004.000000\n",
       "25%      15.000000     8.000000   335.000000     0.000000  1004.000000\n",
       "50%      30.000000    16.000000   534.500000     1.000000  1100.000000\n",
       "75%      45.000000    25.000000   768.000000     2.000000  1178.000000\n",
       "max      59.000000    39.000000   998.000000     2.000000  1178.000000"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I have decided against using strict camelCase, and capatalized the beginning of words too. Partly for aesthetic reasons, but also to avoid stealing conventions from javaScript \n",
    "site_0['SiteViews'] = site_0.shape[0]\n",
    "site_1['SiteViews'] = site_1.shape[0]\n",
    "site_2['SiteViews'] = site_2.shape[0]\n",
    "allViews = pd.concat([site_0, site_1, site_2])\n",
    "assert  len(allViews) > 1, 'Ingested siteVew files appear empty'\n",
    "\n",
    "allViews.timestamp = pd.to_datetime(allViews.timestamp) \n",
    "\n",
    "allViews.Timestamp = pd.to_datetime(allViews.timestamp,format='%y/%m/%d %H:%M:%S')\n",
    "assert (allViews.Timestamp > pd.datetime.now()).sum() < allViews.Timestamp.notna().count(), \"View time appears to contain labels in the future\"\n",
    "\n",
    "if (allViews.Timestamp > pd.datetime.now()).sum() > allViews.Timestamp.notna().count():\n",
    "    print(\"Warining! View time appears to contain labels with future dates\")\n",
    "elif allViews.Timestamp.dt.hour.nunique() > 1:\n",
    "    print(\"Warining! View time contains labels stretching over multiple hours! Consider reformatting code to analyse hours\")\n",
    "\n",
    "elif allViews.Timestamp.dt.date.nunique() >1:\n",
    "        print(\"Warining! View time contains labels stretching over multiple days! Consider reformitting code to analyse dates\")\n",
    "\n",
    "else: \n",
    "    print(\"View time is always labeled with historic dates, as we'd hope! All views appear to be within the same date, within the same hour\")\n",
    "\n",
    "allViews['Second'] = allViews.Timestamp.dt.second\n",
    "allViews['Minute'] = allViews.Timestamp.dt.minute\n",
    "\n",
    "#Could also use replace, someonelse decide \n",
    "allViews.columns = ['ArticleId', 'UserId', 'Timestamp', 'site', 'siteViews', 'Second', 'Minute']\n",
    "allViews = allViews[['Timestamp', 'Second', 'Minute', 'ArticleId', 'UserId', 'site', 'siteViews']]\n",
    "allViews.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T17:01:49.131226Z",
     "start_time": "2021-10-28T17:01:49.064351Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for empty fields, by column \n",
    "if allViews.isna().sum().sum() > 0:\n",
    "    print('Data Contains Null Values')\n",
    "    display('site_0 data null values', site_0.isna().sum(), 'site_1 data null values', site_1.isna().sum(), 'site_2 data null values', site_2.isna().sum(), 'Combined data null values', allViews.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T12:28:21.462267Z",
     "start_time": "2021-10-28T12:28:21.324152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3282 entries, 0 to 1003\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Timestamp  3282 non-null   datetime64[ns]\n",
      " 1   Second     3282 non-null   int64         \n",
      " 2   Minute     3282 non-null   int64         \n",
      " 3   ArticleId  3282 non-null   object        \n",
      " 4   UserId     3282 non-null   int64         \n",
      " 5   site       3282 non-null   int64         \n",
      " 6   siteViews  3282 non-null   int64         \n",
      "dtypes: datetime64[ns](1), int64(5), object(1)\n",
      "memory usage: 381.4 KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3282 entries, 0 to 1003\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Timestamp  3282 non-null   datetime64[ns]\n",
      " 1   Second     3282 non-null   int8          \n",
      " 2   Minute     3282 non-null   int8          \n",
      " 3   ArticleId  3282 non-null   category      \n",
      " 4   UserId     3282 non-null   int16         \n",
      " 5   site       3282 non-null   int8          \n",
      " 6   siteViews  3282 non-null   int16         \n",
      "dtypes: category(1), datetime64[ns](1), int16(2), int8(3)\n",
      "memory usage: 138.2 KB\n"
     ]
    }
   ],
   "source": [
    "'''Although not as relevant to the small data sets you kindly provided, streamed data can become quite large!\n",
    "df.info() will give us some high level information about our dataframe, including its size, information about data types and memory usage.\n",
    "By default, pandas approximates of the memory usage of the dataframe to save time. Because we’re interested in accuracy, we’ll set the memory_usage parameter to 'deep' to get an accurate number.\n",
    "Pandas Int8 ranges between [-128 : 127], and Int16 between [-32768 : 32767]. With the describe method, we know 3/5 of our data types fit into int8 dtypes, 2/5 into int 36. However, other batches may produce different streaming methods.\n",
    "I have therefore used the errors='raise' argument to allert felow SN collegues if data will not fit in the provided type. In such a situation, use a larger datatype. \n",
    "If this results in memory being problematc, we could load in smaller chunks, or use libaries instead of pandas that allow for lazy evaluation, where  computation is executed only when necessary '''\n",
    "\n",
    "allViews.info(memory_usage='deep')\n",
    "\n",
    "allViews.select_dtypes(include ='int64') \n",
    "allViews[['Second', 'Minute', 'site']] = allViews[['Second', 'Minute', 'site']].astype('int8', errors='raise')\n",
    "allViews[['UserId','siteViews']] = allViews[['UserId','siteViews']].astype('int16', errors='raise')\n",
    "\n",
    "allViews[['ArticleId']] = allViews[['ArticleId']].astype('category')\n",
    "\n",
    "allViews.info(memory_usage='deep')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producer to Batch Views in Chronological Order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T15:17:42.677282Z",
     "start_time": "2021-10-28T15:17:42.614302Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Window Analysed:      2020-03-01     12 hour      39 min : 59 sec - 39 min : 0 sec\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def producer(Ceiling_Minute = allViews.Minute.max(), Floor_Minute =  allViews.Minute.max(), Ceiling_Second = allViews.Second.max(), Floor_Second = allViews.Second.min()):\n",
    "\n",
    "\n",
    "    if Ceiling_Minute < Floor_Minute:\n",
    "        raise Exception(\"Sorry, the lowest input minute, the Floor_ Minute, is greater than input Ceiling_ Minute, no views would be returned\")\n",
    "    if Ceiling_Second < Floor_Second:\n",
    "        raise Exception(\"Sorry, the lowest input second, the Floor_ Second, is greater than input Ceiling_ Second, no views would be returned\")\n",
    "\n",
    "    print('Time Window Analysed:     ',  pd.to_datetime(allViews.Timestamp[0]).dt.date.drop_duplicates().to_string(index = False), '   ', pd.to_datetime(allViews.Timestamp[0]).dt.hour.drop_duplicates().to_string(index = False),'hour     ',\n",
    "          Ceiling_Minute, 'min :', Ceiling_Second, 'sec -', Floor_Minute, 'min :', Floor_Second , 'sec')\n",
    "       \n",
    "    \n",
    " #   print('Time Window Analysed:     ',  pd.to_datetime(cSpeak['START DATE/TIME']).date(), Ceiling_Minute, 'min :', Ceiling_Second, 'sec -', Floor_Minute, 'min :', Floor_Second , 'sec')\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    producer.allViewsSorted = allViews.sort_values(by = 'Timestamp')\n",
    "\n",
    "\n",
    "    MinSecond = producer.allViewsSorted['Second'] > Floor_Second\n",
    "    MaxSecond = producer.allViewsSorted['Second'] < Ceiling_Second\n",
    "    MinMinute = producer.allViewsSorted['Minute'] > Floor_Minute\n",
    "    MaxMinute = producer.allViewsSorted['Minute'] < Ceiling_Minute\n",
    "    TimeWindow = producer.allViewsSorted[(MinSecond) ] # & (MaxSecond) ]  # & (MinMinute) & (MaxMinute)]\n",
    "\n",
    "\n",
    "\n",
    "    #Here, I outline a way to return our AllViews dataframe in batches. The first line splits the dataframe into rows of ten using a range loop. In conjustion with the second line, yield i, the producer function will output our AllViews dataframe in batches of ten rows at a time when called upon\n",
    "    for i in range(0,TimeWindow.shape[0],10):\n",
    "        yield i\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# I have not used np.inf here to set default arguments in the producer function defination. If we alter the default arguments in the function call to infinite values, this will mean the function can be used on any input View data. However, would make the code run slighly slower. \n",
    "#I have used the dataset absolutes and shown how infinites can be used below to show both are possible, and defined also infinity in different ways to make the code slighly less boring! \n",
    "# Ceiling_Minute =  np.inf, Floor_Minute = float('-inf'), Ceiling_Second = float('inf'), Floor_Second = float('-inf')\n",
    "\n",
    "next(producer(Ceiling_Minute = allViews.Minute.max(), Floor_Minute =  allViews.Minute.max(), Ceiling_Second = allViews.Second.max(), Floor_Second = allViews.Second.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comsumer to generate insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T15:32:57.911578Z",
     "start_time": "2021-10-28T15:32:57.798881Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Window Analysed:      2020-03-01     12 hour      12 min : 20 sec - 2 min : 2 sec\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Site With Highest Views: 0\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Article With Highest Views: 12-196\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Most Viewed Minute:          10\n",
      "Views in Busiest Minute:    192\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "A Robot is identified when a user is responsible for more than 12 views a second\n",
      " User Id of potential Robot(s):  Suspect Second:  Views in Suspect Second:\n",
      "                            510               45                       101\n"
     ]
    }
   ],
   "source": [
    "def consumer(Ceiling_Minute = 12, Floor_Minute = 8, Ceiling_Second = 20, Floor_Second = 2):\n",
    "    \n",
    "    producer.ViewsSortedBatched = next(producer(Ceiling_Minute = 12, Floor_Minute = 2, Ceiling_Second = 20, Floor_Second = 2))\n",
    "      \n",
    "    \n",
    "\n",
    "\n",
    "    # 1. The site with the most article views\n",
    "\n",
    "    \n",
    "    # Here I create a clone of the Article Id feature of our dataset. This is not necessary for calculations, but does mean we can group one whilst aggregate on the other Id, making outputing a clean verson of our results for part two slighly easier. \n",
    "    #This is not necessary for part one, as we are aggregating on seperate features, nor part three or four, where we cam aggregate on the generic timestamp column, rather than the minute or second features  \n",
    "    producer.allViewsSorted['ArticleIdForCount'] = producer.allViewsSorted['ArticleId'] \n",
    "\n",
    "                                                                                                                #The pandas group by function will return a multi-lolumn index. This can make calling columns, for sorting for example, a little fiddly.\n",
    "                                                                                                                #For legibility, and my efficiency, I like to transpose the groupby result, the reset its index, and transpose back \n",
    "    MaxSiteViewsDF =  producer.allViewsSorted.groupby(['site'], as_index = False).agg({'siteViews': ['count']}).T.reset_index(drop=True).T\n",
    "    MaxSiteViewsDF.columns = ['site', 'MeanSiteViews']\n",
    "\n",
    "    # Here, we sort by the number of site views in each site, highest ontop with the asceding False argument, then extract just the first row. We could also extract the first row with the .head method. \n",
    "    MaxSiteViewsRow = MaxSiteViewsDF.sort_values(by = MaxSiteViewsDF.columns[-1], ascending=False).iloc[:1]\n",
    "    SiteWithMaxView = MaxSiteViewsRow.iloc[:,0]\n",
    "\n",
    "    SiteWithMaxView.columns = ['Site With Highest Views', 'Site Views']\n",
    "    consumer.SingleSiteWithMaxView = SiteWithMaxView.iloc[-1]\n",
    "\n",
    "    print('Site With Highest Views:', consumer.SingleSiteWithMaxView)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    \n",
    "    #2. The most common paper\n",
    "\n",
    "    \n",
    "    MaxArtcleViewsDF = producer.allViewsSorted.groupby(['ArticleId'], as_index = False).agg({'ArticleIdForCount': ['count']}).T.reset_index(drop=True).T\n",
    "    MaxArtcleViewsDF.columns = ['ArticleId', 'ArticleIdForCount']\n",
    "\n",
    "    ArticleIdViewsRow = MaxArtcleViewsDF.sort_values(by = MaxArtcleViewsDF.columns[-1], ascending=False).iloc[:1]\n",
    "    ArticleIdwithMaxView = ArticleIdViewsRow.iloc[:,0]\n",
    "    ArticleIdwithMaxView.columns = ['Article With Highest Views', 'Article Views']\n",
    "    consumer.ArticleIdOnlywithMaxView = ArticleIdwithMaxView.iloc[-1]\n",
    " \n",
    "    \n",
    "    print('Article With Highest Views:', consumer.ArticleIdOnlywithMaxView)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "    \n",
    "    # 3. Busiest 1 minute period\n",
    "    \n",
    "    MaxSecondViewsDF = producer.allViewsSorted.groupby(['Minute'], as_index = False).agg({'Timestamp': ['count']}).T.reset_index(drop=True).T\n",
    "    MaxSecondViewsDF.columns = ['Most Viewed Minute:', 'Views in Busiest Minute:']\n",
    "\n",
    "    SecondViewsRow = MaxSecondViewsDF.sort_values(by = MaxSecondViewsDF.columns[-1], ascending=False).iloc[:1]\n",
    "    consumer.SecondwithMaxView = SecondViewsRow.iloc[-1]\n",
    "\n",
    "    consumer.SecondwithMaxView.columns = ['Minutes With Highest Views', 'Minutes View Counts']\n",
    "\n",
    "    \n",
    "    # I decided to display the minute the the greatest number of views in addition to the views of that minute. SN's sights will go under significantly more stress at peak times, spotting temporal patterns could help prevent problems in the past, or identify the aetiology of known patterns. \n",
    "    # The two string method removes the index from the displayed output. The index returns the dataframe row number, and, although corresponding to time in our sorted dataframe, is unlikely to generate meaningful insights to our user. \n",
    "    print(consumer.SecondwithMaxView.to_string())\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # 4.  Suspected Robot Users - continuous clicks on different articles by the same\n",
    "    \n",
    "\n",
    "    MaxSecondViewsDF = producer.allViewsSorted.groupby(['UserId', 'Second'], as_index = False).agg({'Timestamp': ['count']}).T.reset_index(drop=True).T\n",
    "    MaxSecondViewsDF.columns = ['User Id of potential Robot(s):', 'Suspect Second:', 'Views in Suspect Second:']\n",
    "    consumer.MaxSecondViewsDFSorted = MaxSecondViewsDF.sort_values(by = 'Suspect Second:', ascending=False) #could use .head(1)\n",
    "    \n",
    "    # I decided to display display additional information on robot users - the corresponding second(s) of activitity, and the views in that second.  \n",
    "    # The minute the the greatest number of views in addition to the views of that minute. SN's sights will go under significantly more stress at peak times, spotting temporal patterns could help prevent problems in the past, or identify the aetiology of known patterns. \n",
    "    \n",
    "    ''' I have also explained how I defined a robot user. \n",
    "    We could also define a robot user as having greater clicks in a time window than the user average by a certain quantity, \n",
    "    eg.consumer.MaxSecondViewsDFSorted[consumer.MaxSecondViewsDFSorted['Views in Suspect Second:'] >  (consumer.MaxSecondViewsDFSorted['Views in Suspect Second:'].mean() * 3)] will return information on users with more than three times the sample average '''\n",
    "    \n",
    "    print('A Robot is identified when a user is responsible for more than 12 views a second') \n",
    "    print(consumer.MaxSecondViewsDFSorted[consumer.MaxSecondViewsDFSorted['Views in Suspect Second:'] > 12].to_string(index = False))\n",
    "    if consumer.MaxSecondViewsDFSorted[consumer.MaxSecondViewsDFSorted['Views in Suspect Second:'] > 12]['User Id of potential Robot(s):'].nunique() > 3:\n",
    "        print('Warning - We Appear to have been affected by more than three robots!')\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#percenage of robots\n",
    "    \n",
    "consumer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T15:33:33.033795Z",
     "start_time": "2021-10-28T15:33:32.958701Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Window Analysed:      2020-03-01     12 hour      12 min : 20 sec - 8 min : 2 sec\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Site With Highest Views: 0\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Article With Highest Views: 12-196\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Most Viewed Minute:          10\n",
      "Views in Busiest Minute:    192\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "A Robot is identified when a user viewing articals more than 12 times a second\n",
      " User Id of potential Robot(s):  Suspect Second:  Views in Suspect Second:\n",
      "                            510               45                       101\n"
     ]
    }
   ],
   "source": [
    "# This functon is not necessary! I have stripped out the 'backend' of the producer function above, and extracted the 'output' print statements. I have also included the producer function call as and additional way to define our time constraints.\n",
    "#I decided to split up the function as I felt the consumer function contained a lot of code, and havng just the outputs could make it a little easier to read the notebook. Feel free to skip this cell of course \n",
    "\n",
    "def consume(Ceiling_Minute = allViews.Minute.max(), Floor_Minute =  allViews.Minute.max(), Ceiling_Second = allViews.Second.max(), Floor_Second = allViews.Second.min()):\n",
    "    \n",
    "    producer.ViewsSortedBatched = next(producer(Ceiling_Minute = 12, Floor_Minute = 8, Ceiling_Second = 20, Floor_Second = 2))\n",
    "\n",
    "    \n",
    "    # 1. The site with the most article views\n",
    "\n",
    "    print('Site With Highest Views:', consumer.SingleSiteWithMaxView)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------')\n",
    "    \n",
    "    \n",
    "    #2. The most common paper\n",
    "    \n",
    "    print('Article With Highest Views:', consumer.ArticleIdOnlywithMaxView)\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "    \n",
    "    # 3. Busiest 1 minute period\n",
    "\n",
    "    print(consumer.SecondwithMaxView.to_string())\n",
    "    print('------------------------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # 4.  Suspected Robot Users - continuous clicks on different articles by the same\n",
    "\n",
    "    print('A Robot is identified when a user viewing articals more than 12 times a second') \n",
    "    print(consumer.MaxSecondViewsDFSorted[consumer.MaxSecondViewsDFSorted['Views in Suspect Second:'] > 12].to_string(index = False))\n",
    "    if consumer.MaxSecondViewsDFSorted[consumer.MaxSecondViewsDFSorted['Views in Suspect Second:'] > 12]['User Id of potential Robot(s):'].nunique() > 3:\n",
    "        print('Warning - We Appear to have been affected by more than three robots!')\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "consume(Ceiling_Minute = allViews.Minute.max(), Floor_Minute =  allViews.Minute.max(), Ceiling_Second = allViews.Second.max(), Floor_Second = allViews.Second.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T15:37:07.527943Z",
     "start_time": "2021-10-28T15:37:07.426218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Window Analysed:      2020-03-01     12 hour      12 min : 20 sec - 2 min : 2 sec\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Site With Highest Views: 0\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Article With Highest Views: 12-196\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "Most Viewed Minute:          10\n",
      "Views in Busiest Minute:    192\n",
      "------------------------------------------------------------------------------------------------------------------------------------\n",
      "A Robot is identified when a user is responsible for more than 12 views a second\n",
      " User Id of potential Robot(s):  Suspect Second:  Views in Suspect Second:\n",
      "                            510               45                       101\n"
     ]
    }
   ],
   "source": [
    "    # 5. All of the above (#1-#4) for given time windows.  \n",
    "    # This call is also not necessary for our code to run, or to generate output but I have included to allow easy time window definition, and fuor the purpose of abstraction. We can call the comsume or consumer function and recieve the same output \n",
    "\n",
    "consume(Ceiling_Minute = 12, Floor_Minute = 8, Ceiling_Second = 20, Floor_Second = 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional function to remove outliers from our data\n",
    "### Normally I would remove outliers at the top of our code, before analysis or machine learning is undertaken, but given outlier removal was not specified as a given task, I have included this little function at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T16:08:26.115416Z",
     "start_time": "2021-10-28T16:08:26.047714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possibleColsToTrim Index(['Second', 'Minute', 'UserId', 'site', 'siteViews'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Lists numeric columns we can pick to extract outliers \n",
    "print('possibleColsToTrim', producer.allViewsSorted.select_dtypes('number').columns) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-28T16:54:31.012094Z",
     "start_time": "2021-10-28T16:54:30.929241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waring! - More than 10% of data will be removed with set input columns, Floor and Ceiling!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>siteViews</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Second</th>\n",
       "      <th>Minute</th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>site</th>\n",
       "      <th>siteViews</th>\n",
       "      <th>ArticleIdForCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>553.0</td>\n",
       "      <td>1178</td>\n",
       "      <td>2020-03-01 12:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12-163</td>\n",
       "      <td>553</td>\n",
       "      <td>0</td>\n",
       "      <td>1178</td>\n",
       "      <td>12-163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>336.0</td>\n",
       "      <td>1004</td>\n",
       "      <td>2020-03-01 12:00:01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14-107</td>\n",
       "      <td>336</td>\n",
       "      <td>2</td>\n",
       "      <td>1004</td>\n",
       "      <td>14-107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>888.0</td>\n",
       "      <td>1100</td>\n",
       "      <td>2020-03-01 12:00:01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16-177</td>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1100</td>\n",
       "      <td>16-177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324.0</td>\n",
       "      <td>1178</td>\n",
       "      <td>2020-03-01 12:00:03</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10-125</td>\n",
       "      <td>324</td>\n",
       "      <td>0</td>\n",
       "      <td>1178</td>\n",
       "      <td>10-125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>912.0</td>\n",
       "      <td>1100</td>\n",
       "      <td>2020-03-01 12:00:03</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15-149</td>\n",
       "      <td>912</td>\n",
       "      <td>1</td>\n",
       "      <td>1100</td>\n",
       "      <td>15-149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>237.0</td>\n",
       "      <td>1178</td>\n",
       "      <td>2020-03-01 12:39:19</td>\n",
       "      <td>19</td>\n",
       "      <td>39</td>\n",
       "      <td>13-149</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>1178</td>\n",
       "      <td>13-149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>478.0</td>\n",
       "      <td>1178</td>\n",
       "      <td>2020-03-01 12:39:23</td>\n",
       "      <td>23</td>\n",
       "      <td>39</td>\n",
       "      <td>13-177</td>\n",
       "      <td>478</td>\n",
       "      <td>0</td>\n",
       "      <td>1178</td>\n",
       "      <td>13-177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>311.0</td>\n",
       "      <td>1178</td>\n",
       "      <td>2020-03-01 12:39:25</td>\n",
       "      <td>25</td>\n",
       "      <td>39</td>\n",
       "      <td>13-155</td>\n",
       "      <td>311</td>\n",
       "      <td>0</td>\n",
       "      <td>1178</td>\n",
       "      <td>13-155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>860.0</td>\n",
       "      <td>1178</td>\n",
       "      <td>2020-03-01 12:39:26</td>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>12-100</td>\n",
       "      <td>860</td>\n",
       "      <td>0</td>\n",
       "      <td>1178</td>\n",
       "      <td>12-100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>201.0</td>\n",
       "      <td>1178</td>\n",
       "      <td>2020-03-01 12:39:28</td>\n",
       "      <td>28</td>\n",
       "      <td>39</td>\n",
       "      <td>13-176</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>1178</td>\n",
       "      <td>13-176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2895 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      UserId  siteViews           Timestamp  Second  Minute ArticleId  UserId  \\\n",
       "0      553.0       1178 2020-03-01 12:00:00       0       0    12-163     553   \n",
       "0      336.0       1004 2020-03-01 12:00:01       1       0    14-107     336   \n",
       "0      888.0       1100 2020-03-01 12:00:01       1       0    16-177     888   \n",
       "1      324.0       1178 2020-03-01 12:00:03       3       0    10-125     324   \n",
       "2      912.0       1100 2020-03-01 12:00:03       3       0    15-149     912   \n",
       "...      ...        ...                 ...     ...     ...       ...     ...   \n",
       "1173   237.0       1178 2020-03-01 12:39:19      19      39    13-149     237   \n",
       "1174   478.0       1178 2020-03-01 12:39:23      23      39    13-177     478   \n",
       "1175   311.0       1178 2020-03-01 12:39:25      25      39    13-155     311   \n",
       "1176   860.0       1178 2020-03-01 12:39:26      26      39    12-100     860   \n",
       "1177   201.0       1178 2020-03-01 12:39:28      28      39    13-176     201   \n",
       "\n",
       "      site  siteViews ArticleIdForCount  \n",
       "0        0       1178            12-163  \n",
       "0        2       1004            14-107  \n",
       "0        1       1100            16-177  \n",
       "1        0       1178            10-125  \n",
       "2        1       1100            15-149  \n",
       "...    ...        ...               ...  \n",
       "1173     0       1178            13-149  \n",
       "1174     0       1178            13-177  \n",
       "1175     0       1178            13-155  \n",
       "1176     0       1178            12-100  \n",
       "1177     0       1178            13-176  \n",
       "\n",
       "[2895 rows x 10 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick columns to extract/view outliers  \n",
    "\n",
    "ColsToTrim = [ 'UserId', 'siteViews']\n",
    "\n",
    "  \n",
    "def remove_outlier(ColsToTrim, floor = 0.05, ceiling = 0.95):\n",
    "    df_in = producer.allViewsSorted[ColsToTrim]\n",
    "    q1 = df_in.quantile(floor)\n",
    "    q3 = df_in.quantile(ceiling)\n",
    "    iqr = q3-q1 #Interquartile range\n",
    "    fence_low  = q1-1.5*iqr\n",
    "    fence_high = q3+1.5*iqr\n",
    "    df_trim = df_in[(df_in > fence_low) & (df_in < fence_high)]\n",
    "    df_out = pd.concat([df_trim, producer.allViewsSorted], axis=1).dropna(subset = ColsToTrim)\n",
    "    if len(df_trim) * 10 > len(df_in):\n",
    "        print('Waring! - More than 10% of data will be removed with set input columns, Floor and Ceiling combo!')\n",
    "    return df_out\n",
    "remove_outlier(ColsToTrim, 0.4, 0.65)"
   ]
 
